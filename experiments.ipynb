{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conformalized Survival Analysis for General Right-Censored Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torchtuples as tt\n",
    "import seaborn as sns\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "import random\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from datasets import generate_data, load_dataset\n",
    "from plotting import plot_lpbs_ablation, plot_lpbs, plot_coverage_and_lpb_comperison, plot_c_ind_ibs_and_dcal_comperison, plot_censorship_rate_lpb_coverage, plot_lpb_coverage_n_samples\n",
    "from training import train_models_get_data, get_target, train_early_event_models\n",
    "from calibration import adaptive_conformal_cov, csd_cov, base_model_cov, csd_metrics_estimation, adaptive_conformal_metrics_estimation, base_model_metrics_estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Define the experiment functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_coverage_comparison_same_model(args, n_runs, target_alphas, alphas, frac_early_cens=0.1, threshold_early_cens=0.15, frac_early_surv=0.1, threshold_early_surv=0.12, retrain=True, settings=range(1, 7), include_baselines=True):\n",
    "    index = pd.MultiIndex.from_product(\n",
    "        [['Focused', 'Fused', 'Naive', 'CSD', 'Uncalibrated'], \n",
    "         target_alphas, \n",
    "         settings, \n",
    "         range(n_runs)], \n",
    "        names=['Method', 'Target Alpha', 'Setting', 'Run']\n",
    "    )\n",
    "    \n",
    "    df = pd.DataFrame(index=index, columns=['Coverage', 'A_hat', 'LPB', 'Include Proportion'])\n",
    "    for setting in settings:\n",
    "        print(f'\\n Setting {setting}')\n",
    "        if not retrain:\n",
    "            surv_model, surv_classifier, df_train, df_cal, df_test, x_train, x_cal, x_test = train_models_get_data(setting, args, frac_early_cens=frac_early_cens, threshold_early_cens=threshold_early_cens, frac_early_surv=frac_early_surv, threshold_early_surv=threshold_early_surv)\n",
    "            early_event_model = train_early_event_models(surv_model, alphas, x_train, df_train, setting)\n",
    "        for i in tqdm(range(n_runs)):\n",
    "            if retrain:\n",
    "                surv_model, surv_classifier, df_train, df_cal, df_test, x_train, x_cal, x_test = train_models_get_data(setting, args, frac_early_cens=frac_early_cens, threshold_early_cens=threshold_early_cens, frac_early_surv=frac_early_surv, threshold_early_surv=threshold_early_surv)\n",
    "                early_event_model = train_early_event_models(surv_model, alphas, x_train, df_train, setting)\n",
    "            else:\n",
    "                # Repartition the calibration and test sets\n",
    "                df_cal, df_test, x_cal, x_test = repartition(df_cal, df_test, x_cal, x_test)\n",
    "            durations, events = get_target(df_cal)\n",
    "            if setting in range(1,7):\n",
    "                if include_baselines:\n",
    "                    coverage_base, lengths_base = base_model_cov(target_alphas, setting, surv_model, df_test, x_test)\n",
    "                    coverage_csd, lengths_csd = csd_cov(target_alphas, setting, surv_model, df_train, df_cal, df_test, x_cal, x_test, durations)\n",
    "                coverages_naive, a_hats_naive, lengths_naive = adaptive_conformal_cov(target_alphas, alphas, setting, False, 'naive', surv_model, surv_classifier, df_train, df_cal, df_test, x_train, x_cal, x_test, durations, events)\n",
    "                coverages_focused, a_hats_focused, lengths_focused = adaptive_conformal_cov(target_alphas, alphas, setting, False, 'focus', surv_model, surv_classifier, df_train, df_cal, df_test, x_train, x_cal, x_test, durations, events)\n",
    "                coverages_fused, a_hats_fused, lengths_fused, include_proportion = adaptive_conformal_cov(target_alphas, alphas, setting, False, 'fused', surv_model, surv_classifier, df_train, df_cal, df_test, x_train, x_cal, x_test, durations, events, early_event_model) # s)\n",
    "            else:\n",
    "                if include_baselines:  \n",
    "                    lengths_base = base_model_cov(target_alphas, setting, surv_model, df_test, x_test)\n",
    "                    lengths_csd = csd_cov(target_alphas, setting, surv_model, df_train, df_cal, df_test, x_cal, x_test, durations)\n",
    "                a_hats_naive, lengths_naive = adaptive_conformal_cov(target_alphas, alphas, setting, False, 'naive', surv_model, surv_classifier, df_train, df_cal, df_test, x_train, x_cal, x_test, durations, events)\n",
    "                a_hats_focused, lengths_focused = adaptive_conformal_cov(target_alphas, alphas, setting, False, 'focus', surv_model, surv_classifier, df_train, df_cal, df_test, x_train, x_cal, x_test, durations, events)\n",
    "                a_hats_fused, lengths_fused, include_proportion = adaptive_conformal_cov(target_alphas, alphas, setting, False, 'fused', surv_model, surv_classifier, df_train, df_cal, df_test, x_train, x_cal, x_test, durations, events, early_event_model) # s)\n",
    "\n",
    "            for j, alpha in enumerate(target_alphas):\n",
    "                if include_baselines:\n",
    "                    df.loc[('Uncalibrated', alpha, setting, i), 'LPB'] = lengths_base[j]\n",
    "\n",
    "                    df.loc[('CSD', alpha, setting, i), 'LPB'] = lengths_csd[j]\n",
    "                \n",
    "                df.loc[('Naive', alpha, setting, i), 'A_hat'] = a_hats_naive[j]\n",
    "                df.loc[('Naive', alpha, setting, i), 'LPB'] = lengths_naive[j]\n",
    "\n",
    "                df.loc[('Focused', alpha, setting, i), 'A_hat'] = a_hats_focused[j]\n",
    "                df.loc[('Focused', alpha, setting, i), 'LPB'] = lengths_focused[j]\n",
    "\n",
    "                df.loc[('Fused', alpha, setting, i), 'A_hat'] = a_hats_fused[j]\n",
    "                df.loc[('Fused', alpha, setting, i), 'LPB'] = lengths_fused[j]\n",
    "                df.loc[('Fused', alpha, setting, i), 'Include Proportion'] = include_proportion[j]\n",
    "\n",
    "                if setting in range(1, 7):\n",
    "                    if include_baselines:\n",
    "                        df.loc[('Uncalibrated', alpha, setting, i), 'Coverage'] = coverage_base[j]\n",
    "                        df.loc[('CSD', alpha, setting, i), 'Coverage'] = coverage_csd[j]\n",
    "                    df.loc[('Naive', alpha, setting, i), 'Coverage'] = coverages_naive[j]\n",
    "                    df.loc[('Focused', alpha, setting, i), 'Coverage'] = coverages_focused[j]\n",
    "                    df.loc[('Fused', alpha, setting, i), 'Coverage'] = coverages_fused[j]\n",
    "    return df\n",
    "\n",
    "def run_metrics_comparison_same_model(args, n_runs, target_alphas, alphas, frac_early_cens=0.1, threshold_early_cens=0.15, frac_early_surv=0.1, threshold_early_surv=0.12, retrain=True, settings=range(1, 7)):\n",
    "    index = pd.MultiIndex.from_product(\n",
    "        [['Focused', 'Fused', 'Naive', 'CSD', 'Uncalibrated'], \n",
    "         settings, \n",
    "         range(n_runs)], \n",
    "        names=['Method', 'Setting', 'Run']\n",
    "    )\n",
    "    \n",
    "    df = pd.DataFrame(index=index, columns=['C-Index', 'IBS', 'D-Cal'])\n",
    "    for setting in settings:\n",
    "        print(f'\\n Setting {setting}')\n",
    "        if not retrain:\n",
    "            surv_model, surv_classifier, df_train, df_cal, df_test, x_train, x_cal, x_test = train_models_get_data(setting, args, frac_early_cens=frac_early_cens, threshold_early_cens=threshold_early_cens, frac_early_surv=frac_early_surv, threshold_early_surv=threshold_early_surv)\n",
    "            # early_event_models = train_early_event_models(surv_model, alphas, x_train, df_train, setting)\n",
    "            early_event_model = train_early_event_models(surv_model, alphas, x_train, df_train, setting)\n",
    "        for i in tqdm(range(n_runs)):\n",
    "            if retrain:\n",
    "                surv_model, surv_classifier, df_train, df_cal, df_test, x_train, x_cal, x_test = train_models_get_data(setting, args, frac_early_cens=frac_early_cens, threshold_early_cens=threshold_early_cens, frac_early_surv=frac_early_surv, threshold_early_surv=threshold_early_surv)\n",
    "                # early_event_models = train_early_event_models(surv_model, alphas, x_train, df_train, setting)\n",
    "                early_event_model = train_early_event_models(surv_model, alphas, x_train, df_train, setting)\n",
    "            else:\n",
    "                # Repartition the calibration and test sets\n",
    "                df_cal, df_test, x_cal, x_test = repartition(df_cal, df_test, x_cal, x_test)\n",
    "            durations, events = get_target(df_cal)\n",
    "            c_ind_base, ibs_base, dcal_base = base_model_metrics_estimation(target_alphas, setting, surv_model, df_test, x_test, df_train)\n",
    "            c_ind_csd, ibs_csd, dcal_csd = csd_metrics_estimation(target_alphas, setting, surv_model, df_train, df_cal, df_test, x_cal, x_test, durations)\n",
    "            c_ind_naive, ibs_naive, dcal_naive = adaptive_conformal_metrics_estimation(target_alphas, alphas, setting, False, 'naive', surv_model, surv_classifier, df_train, df_cal, df_test, x_train, x_cal, x_test, durations, events)\n",
    "            c_ind_focused, ibs_focused, dcal_focused = adaptive_conformal_metrics_estimation(target_alphas, alphas, setting, False, 'focus', surv_model, surv_classifier, df_train, df_cal, df_test, x_train, x_cal, x_test, durations, events)\n",
    "            c_ind_fused, ibs_fused, dcal_fused = adaptive_conformal_metrics_estimation(target_alphas, alphas, setting, False, 'fused', surv_model, surv_classifier, df_train, df_cal, df_test, x_train, x_cal, x_test, durations, events, early_event_model) # s)\n",
    "\n",
    "            df.loc[('Uncalibrated', setting, i), 'C-Index'] = c_ind_base\n",
    "            df.loc[('Uncalibrated', setting, i), 'IBS'] = ibs_base\n",
    "            df.loc[('Uncalibrated', setting, i), 'D-Cal'] = dcal_base\n",
    "\n",
    "            df.loc[('CSD', setting, i), 'C-Index'] = c_ind_csd\n",
    "            df.loc[('CSD', setting, i), 'IBS'] = ibs_csd\n",
    "            df.loc[('CSD', setting, i), 'D-Cal'] = dcal_csd\n",
    "\n",
    "            df.loc[('Naive', setting, i), 'C-Index'] = c_ind_naive\n",
    "            df.loc[('Naive', setting, i), 'IBS'] = ibs_naive\n",
    "            df.loc[('Naive', setting, i), 'D-Cal'] = dcal_naive\n",
    "\n",
    "            df.loc[('Focused', setting, i), 'C-Index'] = c_ind_focused\n",
    "            df.loc[('Focused', setting, i), 'IBS'] = ibs_focused\n",
    "            df.loc[('Focused', setting, i), 'D-Cal'] = dcal_focused\n",
    "\n",
    "            df.loc[('Fused', setting, i), 'C-Index'] = c_ind_fused\n",
    "            df.loc[('Fused', setting, i), 'IBS'] = ibs_fused\n",
    "            df.loc[('Fused', setting, i), 'D-Cal'] = dcal_fused\n",
    "    return df\n",
    "\n",
    "def repartition(df_cal, df_test, x_cal, x_test):\n",
    "    # merge df_cal and df_test\n",
    "    df_cal = pd.concat([df_cal, df_test])\n",
    "    df_cal = df_cal.reset_index(drop=True)\n",
    "    # sample 50% of the merged df\n",
    "    df_test = df_cal.sample(frac=0.5)\n",
    "    df_cal = df_cal.drop(df_test.index)\n",
    "    # merge x_cal and x_test\n",
    "    x_cal = np.concatenate([x_cal, x_test])\n",
    "    x_test = x_cal[df_test.index]\n",
    "    x_cal = np.delete(x_cal, df_test.index, axis=0)\n",
    "    return df_cal, df_test, x_cal, x_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Synthetic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\n",
    "    'n_samples': 2000,\n",
    "    'num_nodes': [5],\n",
    "    'batch_norm': 'batch',\n",
    "    'dropout': 0.1,\n",
    "    'batch_size': 256,\n",
    "    'lr': .002,\n",
    "    'epochs': 50,\n",
    "    'callbacks': [tt.callbacks.EarlyStopping(patience=5)],\n",
    "    'verbose': False\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set seeds for reproducibility\n",
    "random.seed(12)\n",
    "np.random.seed(12)\n",
    "_ = torch.manual_seed(12)\n",
    "alphas = np.logspace(-2, -1, 30)\n",
    "target_alphas = np.round(np.array([0.1]), 5)\n",
    "\n",
    "\n",
    "df_synthetic = run_coverage_comparison_same_model(args, 20, target_alphas, alphas, retrain=True, settings=range(1,7), frac_early_surv=0.05, frac_early_cens=0.12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the function with your DataFrame and target alpha\n",
    "plot_coverage_and_lpb_comperison(df_synthetic, 0.1, fontsize=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alphas = np.linspace(0.02, 0.98, 98)\n",
    "# target_alphas = np.round(np.linspace(0.1, 0.9, 20), 10)\n",
    "# df_metrics_synth = run_metrics_comparison_same_model(args, 5, target_alphas, alphas, retrain=False, settings=range(1,7), frac_early_surv=0.05, frac_early_cens=0.12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_c_ind_ibs_and_dcal_comperison(df_metrics_synth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TCGA-BRCA data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "args_real = {\n",
    "    'num_nodes': [32, 32],\n",
    "    'batch_norm': 'batch',\n",
    "    'dropout': 0.1,\n",
    "    'batch_size': 256,\n",
    "    'lr': .002,\n",
    "    'epochs': 1000,\n",
    "    'callbacks': [tt.callbacks.EarlyStopping(patience=5)],\n",
    "    'verbose': False\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seeds for reproducibility\n",
    "random.seed(12)\n",
    "np.random.seed(12)\n",
    "_ = torch.manual_seed(12)\n",
    "\n",
    "alphas = np.logspace(-2, -1, 30)\n",
    "target_alphas = np.round(np.array([0.1]), 5)\n",
    "\n",
    "df_real = run_coverage_comparison_same_model(args_real, 100, target_alphas, alphas, retrain=False, settings=['tcga', 'support', 'metabric', 'churn', 'nacd', 'gbsg'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print include proportion for the Fused method\n",
    "print(df_real.groupby(['Method', 'Setting'])['Include Proportion'].mean().unstack())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_lpbs(df_real, 0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alphas = np.linspace(0.02, 0.98, 98)\n",
    "# target_alphas = np.round(np.linspace(0.1, 0.9, 10), 10)\n",
    "# df_metrics_real = run_metrics_comparison_same_model(args_real, 5, target_alphas, alphas, retrain=False, settings=['support', 'metabric', 'churn', 'tcga', 'nacd', 'gbsg'])\n",
    "# plot_c_ind_ibs_and_dcal_comperison(df_metrics_real)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparams ablation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_coverage_comparison_ablation(args, n_runs, target_alphas, alphas, frac_early_cens=0.1, threshold_early_cens=0.15, frac_early_surv=0.1, threshold_early_surv=0.12, retrain=True, settings=range(1, 7)):\n",
    "    dqs = {'Shallow q': [5], 'Deep q': [5, 5, 5]}\n",
    "    dws = {'Shallow w': 2, 'Deep w': 6}\n",
    "    index = pd.MultiIndex.from_product(\n",
    "        [['Focused', 'Fused', 'Naive'], \n",
    "         target_alphas, \n",
    "         settings, \n",
    "         range(n_runs),\n",
    "         dqs.keys(),\n",
    "         dws.keys()],\n",
    "        names=['Method', 'Target Alpha', 'Setting', 'Run', 'Depth Quantiles', 'Depth Weights']\n",
    "    )\n",
    "    df = pd.DataFrame(index=index, columns=['Coverage', 'A_hat', 'LPB', 'Include Proportion'])\n",
    "    for setting in settings:\n",
    "        print(f'\\n Setting {setting}')\n",
    "        for dq in dqs.keys():\n",
    "            for dw in dws.keys():\n",
    "                args[\"num_nodes\"] = dqs[dq]\n",
    "                if not retrain:\n",
    "                    surv_model, surv_classifier, df_train, df_cal, df_test, x_train, x_cal, x_test = train_models_get_data(setting, args, frac_early_cens=frac_early_cens, threshold_early_cens=threshold_early_cens, frac_early_surv=frac_early_surv, threshold_early_surv=threshold_early_surv, max_depth_w=dws[dw])\n",
    "                    early_event_model = train_early_event_models(surv_model, alphas, x_train, df_train, setting)\n",
    "                for i in tqdm(range(n_runs)):\n",
    "                    if retrain:\n",
    "                        surv_model, surv_classifier, df_train, df_cal, df_test, x_train, x_cal, x_test = train_models_get_data(setting, args, frac_early_cens=frac_early_cens, threshold_early_cens=threshold_early_cens, frac_early_surv=frac_early_surv, threshold_early_surv=threshold_early_surv, max_depth_w=dws[dw])\n",
    "                        early_event_model = train_early_event_models(surv_model, alphas, x_train, df_train, setting)\n",
    "                    else:\n",
    "                        # Repartition the calibration and test sets\n",
    "                        df_cal, df_test, x_cal, x_test = repartition(df_cal, df_test, x_cal, x_test)\n",
    "                    durations, events = get_target(df_cal)\n",
    "                    if setting in range(1,7):\n",
    "                        coverages_naive, a_hats_naive, lengths_naive = adaptive_conformal_cov(target_alphas, alphas, setting, False, 'naive', surv_model, surv_classifier, df_train, df_cal, df_test, x_train, x_cal, x_test, durations, events)\n",
    "                        coverages_focused, a_hats_focused, lengths_focused = adaptive_conformal_cov(target_alphas, alphas, setting, False, 'focus', surv_model, surv_classifier, df_train, df_cal, df_test, x_train, x_cal, x_test, durations, events)\n",
    "                        coverages_fused, a_hats_fused, lengths_fused, include_proportion = adaptive_conformal_cov(target_alphas, alphas, setting, False, 'fused', surv_model, surv_classifier, df_train, df_cal, df_test, x_train, x_cal, x_test, durations, events, early_event_model) # s)\n",
    "                    else:\n",
    "                        a_hats_naive, lengths_naive = adaptive_conformal_cov(target_alphas, alphas, setting, False, 'naive', surv_model, surv_classifier, df_train, df_cal, df_test, x_train, x_cal, x_test, durations, events)\n",
    "                        a_hats_focused, lengths_focused = adaptive_conformal_cov(target_alphas, alphas, setting, False, 'focus', surv_model, surv_classifier, df_train, df_cal, df_test, x_train, x_cal, x_test, durations, events)\n",
    "                        a_hats_fused, lengths_fused, include_proportion = adaptive_conformal_cov(target_alphas, alphas, setting, False, 'fused', surv_model, surv_classifier, df_train, df_cal, df_test, x_train, x_cal, x_test, durations, events, early_event_model) # s)\n",
    "\n",
    "                    for j, alpha in enumerate(target_alphas):\n",
    "                        df.loc[('Naive', alpha, setting, i, dq, dw), 'A_hat'] = a_hats_naive[j]\n",
    "                        df.loc[('Naive', alpha, setting, i, dq, dw), 'LPB'] = lengths_naive[j]\n",
    "\n",
    "                        df.loc[('Focused', alpha, setting, i, dq, dw), 'A_hat'] = a_hats_focused[j]\n",
    "                        df.loc[('Focused', alpha, setting, i, dq, dw), 'LPB'] = lengths_focused[j]\n",
    "\n",
    "                        df.loc[('Fused', alpha, setting, i, dq, dw), 'A_hat'] = a_hats_fused[j]\n",
    "                        df.loc[('Fused', alpha, setting, i, dq, dw), 'LPB'] = lengths_fused[j]\n",
    "                        df.loc[('Fused', alpha, setting, i, dq, dw), 'Include Proportion'] = include_proportion[j]\n",
    "\n",
    "                        if setting in range(1, 7):\n",
    "                            df.loc[('Naive', alpha, setting, i, dq, dw), 'Coverage'] = coverages_naive[j]\n",
    "                            df.loc[('Focused', alpha, setting, i, dq, dw), 'Coverage'] = coverages_focused[j]\n",
    "                            df.loc[('Fused', alpha, setting, i, dq, dw), 'Coverage'] = coverages_fused[j]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set seeds for reproducibility\n",
    "random.seed(1)\n",
    "np.random.seed(1)\n",
    "_ = torch.manual_seed(1)\n",
    "alphas = np.linspace(0.02, 0.15, 60)\n",
    "target_alphas = np.round(np.array([0.1]), 5)\n",
    "\n",
    "df_ablation = run_coverage_comparison_ablation(args, 10, target_alphas, alphas, retrain=True, settings=range(3,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the function with the DataFrame and target alpha value\n",
    "plot_lpbs_ablation(df_ablation, 0.1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Censorship proportion experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_comparison_censorship(args, n_runs, target_alphas, alphas, frac_early_cens=0.1, threshold_early_cens=0.15, frac_early_surv=0.1, threshold_early_surv=0.12, retrain=True, settings=range(1, 7)):\n",
    "    end_of_trial = np.linspace(0.5, 10, 10)\n",
    "    index = pd.MultiIndex.from_product(\n",
    "        [['Focused', 'Fused', 'Naive'], \n",
    "         target_alphas, \n",
    "         settings, \n",
    "         range(n_runs),\n",
    "         end_of_trial],\n",
    "        names=['Method', 'Target Alpha', 'Setting', 'Run', 'End of trial time']\n",
    "    )\n",
    "    df = pd.DataFrame(index=index, columns=['Coverage', 'A_hat', 'LPB', 'Include Proportion', 'Censorship Rate'])\n",
    "    for setting in settings:\n",
    "        print(f'\\n Setting {setting}')\n",
    "        for eot in end_of_trial:\n",
    "            if not retrain:\n",
    "                surv_model, surv_classifier, df_train, df_cal, df_test, x_train, x_cal, x_test = train_models_get_data(setting, args, frac_early_cens=frac_early_cens, threshold_early_cens=threshold_early_cens, frac_early_surv=frac_early_surv, threshold_early_surv=threshold_early_surv, eot=eot)\n",
    "                early_event_model = train_early_event_models(surv_model, alphas, x_train, df_train, setting)\n",
    "            for i in tqdm(range(n_runs)):\n",
    "                if retrain:\n",
    "                    surv_model, surv_classifier, df_train, df_cal, df_test, x_train, x_cal, x_test = train_models_get_data(setting, args, frac_early_cens=frac_early_cens, threshold_early_cens=threshold_early_cens, frac_early_surv=frac_early_surv, threshold_early_surv=threshold_early_surv, eot=eot)\n",
    "                    early_event_model = train_early_event_models(surv_model, alphas, x_train, df_train, setting)\n",
    "                else:\n",
    "                    # Repartition the calibration and test sets\n",
    "                    df_cal, df_test, x_cal, x_test = repartition(df_cal, df_test, x_cal, x_test)\n",
    "                durations, events = get_target(df_cal)\n",
    "                if setting in range(1,7):\n",
    "                    coverages_naive, a_hats_naive, lengths_naive = adaptive_conformal_cov(target_alphas, alphas, setting, False, 'naive', surv_model, surv_classifier, df_train, df_cal, df_test, x_train, x_cal, x_test, durations, events)\n",
    "                    coverages_focused, a_hats_focused, lengths_focused = adaptive_conformal_cov(target_alphas, alphas, setting, False, 'focus', surv_model, surv_classifier, df_train, df_cal, df_test, x_train, x_cal, x_test, durations, events)\n",
    "                    coverages_fused, a_hats_fused, lengths_fused, include_proportion = adaptive_conformal_cov(target_alphas, alphas, setting, False, 'fused', surv_model, surv_classifier, df_train, df_cal, df_test, x_train, x_cal, x_test, durations, events, early_event_model) # s)\n",
    "                else:\n",
    "                    a_hats_naive, lengths_naive = adaptive_conformal_cov(target_alphas, alphas, setting, False, 'naive', surv_model, surv_classifier, df_train, df_cal, df_test, x_train, x_cal, x_test, durations, events)\n",
    "                    a_hats_focused, lengths_focused = adaptive_conformal_cov(target_alphas, alphas, setting, False, 'focus', surv_model, surv_classifier, df_train, df_cal, df_test, x_train, x_cal, x_test, durations, events)\n",
    "                    a_hats_fused, lengths_fused, include_proportion = adaptive_conformal_cov(target_alphas, alphas, setting, False, 'fused', surv_model, surv_classifier, df_train, df_cal, df_test, x_train, x_cal, x_test, durations, events, early_event_model) # s)\n",
    "\n",
    "                for j, alpha in enumerate(target_alphas):\n",
    "                    df.loc[('Naive', alpha, setting, i, eot), 'A_hat'] = a_hats_naive[j]\n",
    "                    df.loc[('Naive', alpha, setting, i, eot), 'LPB'] = lengths_naive[j]\n",
    "                    df.loc[('Naive', alpha, setting, i, eot), 'Censorship Rate'] = 1 - df_train['event'].mean()\n",
    "\n",
    "                    df.loc[('Focused', alpha, setting, i, eot), 'A_hat'] = a_hats_focused[j]\n",
    "                    df.loc[('Focused', alpha, setting, i, eot), 'LPB'] = lengths_focused[j]\n",
    "                    df.loc[('Focused', alpha, setting, i, eot), 'Censorship Rate'] = 1 - df_train['event'].mean()\n",
    "\n",
    "                    df.loc[('Fused', alpha, setting, i, eot), 'A_hat'] = a_hats_fused[j]\n",
    "                    df.loc[('Fused', alpha, setting, i, eot), 'LPB'] = lengths_fused[j]\n",
    "                    df.loc[('Fused', alpha, setting, i, eot), 'Include Proportion'] = include_proportion[j]\n",
    "                    df.loc[('Fused', alpha, setting, i, eot), 'Censorship Rate'] = 1 - df_train['event'].mean()\n",
    "\n",
    "                    if setting in range(1, 7):\n",
    "                        df.loc[('Naive', alpha, setting, i, eot), 'Coverage'] = coverages_naive[j]\n",
    "                        df.loc[('Focused', alpha, setting, i, eot), 'Coverage'] = coverages_focused[j]\n",
    "                        df.loc[('Fused', alpha, setting, i, eot), 'Coverage'] = coverages_fused[j]\n",
    "    return df\n",
    "\n",
    "# set seeds for reproducibility\n",
    "random.seed(1)\n",
    "np.random.seed(1)\n",
    "_ = torch.manual_seed(1)\n",
    "alphas = np.linspace(0.02, 0.15, 60)\n",
    "target_alphas = np.round(np.array([0.1]), 5)\n",
    "\n",
    "df_censorship = run_comparison_censorship(args, 10, target_alphas, alphas, retrain=True, settings=range(3,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the function with the DataFrame and target alpha value\n",
    "plot_censorship_rate_lpb_coverage(df_censorship, 0.1, 17)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample num experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_comparison_samples(args, n_runs, target_alphas, alphas, frac_early_cens=0.1, threshold_early_cens=0.15, frac_early_surv=0.1, threshold_early_surv=0.12, retrain=True, settings=range(1, 7)):\n",
    "    num_samples = [200, 500, 800, 1100, 1400, 1700, 2000]\n",
    "    index = pd.MultiIndex.from_product(\n",
    "        [['Focused', 'Fused', 'Naive'], \n",
    "         target_alphas, \n",
    "         settings, \n",
    "         range(n_runs),\n",
    "         num_samples],\n",
    "        names=['Method', 'Target Alpha', 'Setting', 'Run', 'Num Samples']\n",
    "    )\n",
    "    df = pd.DataFrame(index=index, columns=['Coverage', 'A_hat', 'LPB', 'Include Proportion', 'Censorship Rate'])\n",
    "    for setting in settings:\n",
    "        print(f'\\n Setting {setting}')\n",
    "        for n_samples in num_samples:\n",
    "            args['n_samples'] = n_samples\n",
    "            if not retrain:\n",
    "                surv_model, surv_classifier, df_train, df_cal, df_test, x_train, x_cal, x_test = train_models_get_data(setting, args, frac_early_cens=frac_early_cens, threshold_early_cens=threshold_early_cens, frac_early_surv=frac_early_surv, threshold_early_surv=threshold_early_surv)\n",
    "                early_event_model = train_early_event_models(surv_model, alphas, x_train, df_train, setting)\n",
    "            for i in tqdm(range(n_runs)):\n",
    "                if retrain:\n",
    "                    surv_model, surv_classifier, df_train, df_cal, df_test, x_train, x_cal, x_test = train_models_get_data(setting, args, frac_early_cens=frac_early_cens, threshold_early_cens=threshold_early_cens, frac_early_surv=frac_early_surv, threshold_early_surv=threshold_early_surv)\n",
    "                    early_event_model = train_early_event_models(surv_model, alphas, x_train, df_train, setting)\n",
    "                else:\n",
    "                    # Repartition the calibration and test sets\n",
    "                    df_cal, df_test, x_cal, x_test = repartition(df_cal, df_test, x_cal, x_test)\n",
    "                durations, events = get_target(df_cal)\n",
    "                if setting in range(1,7):\n",
    "                    coverages_naive, a_hats_naive, lengths_naive = adaptive_conformal_cov(target_alphas, alphas, setting, False, 'naive', surv_model, surv_classifier, df_train, df_cal, df_test, x_train, x_cal, x_test, durations, events)\n",
    "                    coverages_focused, a_hats_focused, lengths_focused = adaptive_conformal_cov(target_alphas, alphas, setting, False, 'focus', surv_model, surv_classifier, df_train, df_cal, df_test, x_train, x_cal, x_test, durations, events)\n",
    "                    coverages_fused, a_hats_fused, lengths_fused, include_proportion = adaptive_conformal_cov(target_alphas, alphas, setting, False, 'fused', surv_model, surv_classifier, df_train, df_cal, df_test, x_train, x_cal, x_test, durations, events, early_event_model) # s)\n",
    "                else:\n",
    "                    a_hats_naive, lengths_naive = adaptive_conformal_cov(target_alphas, alphas, setting, False, 'naive', surv_model, surv_classifier, df_train, df_cal, df_test, x_train, x_cal, x_test, durations, events)\n",
    "                    a_hats_focused, lengths_focused = adaptive_conformal_cov(target_alphas, alphas, setting, False, 'focus', surv_model, surv_classifier, df_train, df_cal, df_test, x_train, x_cal, x_test, durations, events)\n",
    "                    a_hats_fused, lengths_fused, include_proportion = adaptive_conformal_cov(target_alphas, alphas, setting, False, 'fused', surv_model, surv_classifier, df_train, df_cal, df_test, x_train, x_cal, x_test, durations, events, early_event_model) # s)\n",
    "\n",
    "                for j, alpha in enumerate(target_alphas):\n",
    "                    df.loc[('Naive', alpha, setting, i, n_samples), 'A_hat'] = a_hats_naive[j]\n",
    "                    df.loc[('Naive', alpha, setting, i, n_samples), 'LPB'] = lengths_naive[j]\n",
    "                    df.loc[('Naive', alpha, setting, i, n_samples), 'Censorship Rate'] = 1 - df_train['event'].mean()\n",
    "\n",
    "                    df.loc[('Focused', alpha, setting, i, n_samples), 'A_hat'] = a_hats_focused[j]\n",
    "                    df.loc[('Focused', alpha, setting, i, n_samples), 'LPB'] = lengths_focused[j]\n",
    "                    df.loc[('Focused', alpha, setting, i, n_samples), 'Censorship Rate'] = 1 - df_train['event'].mean()\n",
    "\n",
    "                    df.loc[('Fused', alpha, setting, i, n_samples), 'A_hat'] = a_hats_fused[j]\n",
    "                    df.loc[('Fused', alpha, setting, i, n_samples), 'LPB'] = lengths_fused[j]\n",
    "                    df.loc[('Fused', alpha, setting, i, n_samples), 'Include Proportion'] = include_proportion[j]\n",
    "                    df.loc[('Fused', alpha, setting, i, n_samples), 'Censorship Rate'] = 1 - df_train['event'].mean()\n",
    "\n",
    "                    if setting in range(1, 7):\n",
    "                        df.loc[('Naive', alpha, setting, i, n_samples), 'Coverage'] = coverages_naive[j]\n",
    "                        df.loc[('Focused', alpha, setting, i, n_samples), 'Coverage'] = coverages_focused[j]\n",
    "                        df.loc[('Fused', alpha, setting, i, n_samples), 'Coverage'] = coverages_fused[j]\n",
    "    return df\n",
    "\n",
    "# set seeds for reproducibility\n",
    "random.seed(1)\n",
    "np.random.seed(1)\n",
    "_ = torch.manual_seed(1)\n",
    "alphas = np.linspace(0.02, 0.15, 60)\n",
    "target_alphas = np.round(np.array([0.1]), 5)\n",
    "\n",
    "df_samples = run_comparison_samples(args, 10, target_alphas, alphas, retrain=True, settings=range(3,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the function with the DataFrame and target alpha value\n",
    "plot_lpb_coverage_n_samples(df_samples, 0.1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conformal_surv_general",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
